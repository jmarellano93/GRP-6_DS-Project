# Load libraries
'''
```{r}
library(tidyverse)
library(corrplot)
library(fastDummies)
library(dplyr)
library(ggplot2)
```

# Read the dataset
```{r}
original <- read.csv("~/Documents/Repos/GRP-6_DS-Project/DS-Project_data/LCdata.csv", 
                 row.names = NULL, 
                 sep = ";",
                 header = TRUE)

# Predefined columns to drop
columns_to_drop <- c("collection_recovery_fee",
                     "installment",
                     "funded_amnt",
                     "funded_amnt_inv",
                     "issue_d",
                     "last_pymnt_amnt",
                     "last_pymnt_d",
                     "loan_status",
                     "next_pymnt_d",
                     "out_prncp",
                     "out_prncp_inv",
                     "pymnt_plan",
                     "recoveries",
                     "total_pymnt",
                     "total_pymnt_inv",
                     "total_rec_int",
                     "total_rec_late_fee",
                     "total_rec_prncp")
original_drop <- original[, -match(columns_to_drop,  names(original))]

view(original_drop)
```


```{r}
# Dtype dictionary
dtype_dict <- list(
  acc_now_delinq = "integer",
  addr_state = "factor",
  all_util = "numeric",
  annual_inc = "numeric",
  annual_inc_joint = "numeric",
  application_type = "factor",
  collections_12_mths_ex_med = "integer",
  delinq_2yrs = "integer",
  desc = "factor",
  dti = "numeric",
  dti_joint = "numeric",
  earliest_cr_line = "date",
  emp_length = "factor",
  emp_title = "factor",
  home_ownership = "factor",
  id = "integer",
  il_util = "numeric",
  initial_list_status = "factor",
  inq_fi = "integer",
  inq_last_12m = "integer",
  inq_last_6mths = "integer",
  int_rate = "numeric",
  last_credit_pull_d = "date",
  loan_amnt = "integer",
  max_bal_bc = "numeric",
  member_id = "integer",
  mths_since_last_delinq = "integer",
  mths_since_last_major_derog = "integer",
  mths_since_last_record = "integer",
  mths_since_rcnt_il = "integer",
  open_acc = "integer",
  open_acc_6m = "integer",
  open_il_12m = "integer",
  open_il_24m = "integer",
  open_il_6m = "integer",
  open_rv_12m = "integer",
  open_rv_24m = "integer",
  policy_code = "factor",
  pub_rec = "integer",
  purpose = "factor",
  revol_bal = "integer",
  revol_util = "numeric",
  term = "factor",
  title = "factor",
  tot_coll_amt = "numeric",
  tot_cur_bal = "numeric",
  total_acc = "integer",
  total_bal_il = "numeric",
  total_cu_tl = "integer",
  total_rev_hi_lim = "numeric",
  url = "factor",
  zip_code = "factor"
)
```

```{r}
# Cast dtypes according to dictionary
for(col_name in names(dtype_dict)) {
  dtype <- dtype_dict[[col_name]]
  if(dtype == "factor") {
    original_drop[[col_name]] <- as.factor(original_drop[[col_name]])
  } else if(dtype == "integer") {
    original_drop[[col_name]] <- as.integer(original_drop[[col_name]])
  } else if(dtype == "numeric") {
    original_drop[[col_name]] <- as.numeric(original_drop[[col_name]])
  } else if(dtype == "date") {
    original_drop[[col_name]] <- as.Date(original_drop[[col_name]], format="%b-%Y")
  }
}

str(original_drop)
```

```{r}
# Make Copy to work with

data <- original_drop
```

# Basic exploration
```{r}
dim(data) # The shape is 798641:54

# Display the structure of the dataset
str(data)
view(data)

# Display the first few rows of the dataset
head(data)

# Display summary statistics of the dataset
summary(data)

# Search for duplicated rows
dup_ids <- data[duplicated(data$id), ]
dup_ids # There are no duplicated rows
```

# NA Analysis
```{r}
# Which columns has NA
na_counts <- colSums(is.na(data))
na_counts[na_counts > 0]

# Drop rows which ONLY have NA
# 1. Get the total number of columns in the dataframe
num_cols <- ncol(data)
  
# 2. Calculate the number of NAs per row
na_count_per_row <- rowSums(is.na(data))
  
# 3. Filter the dataframe to keep only rows where the NA count is LESS THAN the total number of columns (i.e., not ALL are NA)
data_clean <- data[na_count_per_row < num_cols, ]
  
# To check the result:
print(paste("Original rows:", nrow(data)))
print(paste("Rows remaining:", nrow(data_clean)))
  
data <- data_clean
```


# -----------------------------
# Identify NA and Missingness
# -----------------------------



# Features which can be removed right away
pre_select_feat_drop <- c("member_id", # already one identifier called id, id's do not have predictive power
                          "url", # linking to the loan listing; provides no predictive power
                          "zip_code" # too granular and often redundant with addr_state
                          )
feat_to_drop <- c(pre_select_feat_drop)

```{r}

# ===============================================================
# Inspect missingness in installment loan-related features
# ===============================================================
# 1. Define columns of interest
rct_il_features <- c("id", "il_util", "total_bal_il",
                     "open_il_12m", "open_il_24m", "open_il_6m", "mths_since_rcnt_il")

il_cols <- c("open_il_12m", "open_il_24m", "open_il_6m", "mths_since_rcnt_il")

# 2. Calculate NA count per column
na_summary <- colSums(is.na(data[, rct_il_features]))
na_percent <- round(na_summary / nrow(data) * 100, 2)

na_overview <- data.frame(
  column = names(na_summary),
  na_count = na_summary,
  na_percent = na_percent
)

cat("=== NA Summary per Installment-Related Column ===\n")
print(na_overview)

# 3. Calculate how many IL columns are NA per row
is_na_matrix <- is.na(data[, il_cols])
na_count_per_row <- rowSums(is_na_matrix)

# 4. Count how many rows have 0–6 NAs in these columns
row_na_distribution <- table(na_count_per_row)

# 5. Compute percentages
row_na_percentage <- round(100 * row_na_distribution / nrow(data), 2)

na_pattern_summary <- data.frame(
  n_na_columns = names(row_na_distribution),
  n_rows = as.numeric(row_na_distribution),
  percent_of_total = as.numeric(row_na_percentage)
)

cat("\n=== NA Pattern Across Rows (Installment Features) ===\n")
print(na_pattern_summary)

# 6. Identify rows that could be imputed (1–4 NAs among IL columns)
il_rows_for_impute <- data[na_count_per_row >= 1 & na_count_per_row < length(il_cols), 
                           c("id", il_cols)]
nr_il_rows_for_impute <- nrow(il_rows_for_impute)
cat("\nRows with partial installment data (1–4 NAs):", nr_il_rows_for_impute, "\n")

# 7. Identify rows with all IL columns NA (no installment info at all)
n_rows_all_il_na <- sum(na_count_per_row == length(il_cols))
cat("Rows with all installment-related columns NA:", n_rows_all_il_na, "\n")

# 8.1 Rows where all IL columns NA but il_util is NOT NA
rows_all_il_na_ilutil_notna <- data[na_count_per_row == length(il_cols) & 
                                      !is.na(data$il_util), 
                                    c("id", "il_util", il_cols)]
n_rows_all_il_na_ilutil_notna <- nrow(rows_all_il_na_ilutil_notna)
cat("Rows with all IL columns NA but il_util present:", n_rows_all_il_na_ilutil_notna, "\n")

# 8.2. Rows where all IL columns NA but total_bal_il is NOT NA
rows_all_il_na_totalbalil_notna <- data[na_count_per_row == length(il_cols) & 
                                      !is.na(data$total_bal_il), 
                                    c("id", "total_bal_il", il_cols)]
n_rows_all_il_na_totalbalil_notna <- nrow(rows_all_il_na_totalbalil_notna)
cat("Rows with all IL columns NA but total_bal_il present:", n_rows_all_il_na_totalbalil_notna, "\n")

# 9. Add a flag column to data (1 = at least one IL feature available, 0 = all missing)
data$has_installment_data <- ifelse(na_count_per_row < length(il_cols), 1, 0)

cat("\nNew flag column 'has_installment_data' created.\n")
cat("Rows with installment data:", sum(data$has_installment_data == 1), "\n")
cat("Rows without installment data:", sum(data$has_installment_data == 0), "\n")

# 10. Total number of rows
total_rows <- nrow(data)
cat("\nTotal rows in dataset:", total_rows, "\n")

# Add installment loan columns to drop list
feat_to_drop <- c(feat_to_drop, il_cols)

```


# NA percentage per column 
```{r}
na_df <- data.frame(
  column = names(data),
  na_percent = round(colMeans(is.na(data)) * 100,2)
)
print(na_df)
```

```{r}
# Identify columns with > 90% NA and drop it
cols_to_remove <- na_df$column[na_df$na_percent > 90]
cols_to_remove

data_filtered <- data[ , !names(data) %in% cols_to_remove]
```



# Distribution of the data
```{r}
plot_numeric_distributions <- function(data, target_variable, bins = 30) {
  
  # 1. Select all numeric columns, excluding the specified target variable
  # Note: The `{{target_variable}}` syntax is required when using the `dplyr` 'tidy evaluation' style 
  # with variables passed as arguments.
  numerical_vars <- data %>%
    select(where(is.numeric), -all_of(target_variable)) %>% 
    names()
  
  if (length(numerical_vars) == 0) {
    message("No numerical variables found after excluding the target.")
    return(invisible(NULL))
  }
  
  # 2. Loop through the remaining numerical variables and generate histograms
  for (col in numerical_vars) {
    
    # Use !!sym(col) for non-standard evaluation with ggplot2 inside a loop/function
    p <- ggplot(data, aes(x = !!sym(col))) +
      geom_histogram(bins = bins, fill = "darkblue", color = "white") +
      labs(
        title = paste("Distribution of", col), 
        x = col, 
        y = "Frequency"
      ) +
      theme_minimal()
    
    print(p)
  }
}

# Example usage:
plot_numeric_distributions(data = data_filtered, target_variable = "int_rate")
```
```{r}
# Boxplots to Identify Outliers

plot_numeric_boxplots <- function(data, target_variable) {
  
  # 1. Select all numeric columns, excluding the specified target variable
  numerical_vars <- data %>%
    select(where(is.numeric), -all_of(target_variable)) %>% 
    names()
  
  if (length(numerical_vars) == 0) {
    message("No numerical variables found after excluding the target.")
    return(invisible(NULL))
  }
  
  # 2. Loop through the remaining numerical variables and generate boxplots
  for (col in numerical_vars) {
    
    # Use !!sym(col) for non-standard evaluation with ggplot2 inside a loop/function
    p <- ggplot(data, aes(y = !!sym(col))) +
      # Use a single, central column for the boxplot aesthetic
      geom_boxplot(fill = "lightgreen", color = "darkgreen") +
      labs(
        title = paste("Boxplot of", col), 
        y = col
      ) +
      # Adding this line helps in visualizing the boxplot clearly when there's only one group
      theme_minimal() +
      theme(
        axis.title.x = element_blank(), # Hide x-axis label
        axis.text.x = element_blank(),  # Hide x-axis ticks/text
        axis.ticks.x = element_blank() # Hide x-axis ticks
      )
    
    print(p)
  }
}

# Example usage:
plot_numeric_boxplots(data = data_filtered, target_variable = "int_rate")

```

```{r}
# Loan_amnt has a spike at upper range
# This could be due to lending caps

# Find the top 10 most frequent loan amounts
top_loan_amounts <- data_filtered %>%
  # Filter for loans in the upper range (e.g., above $30,000)
  filter(loan_amnt >= 30000) %>% 
  count(loan_amnt, sort = TRUE) %>%
  head(20)

print(top_loan_amounts) # Lending cap is found at 35000, be aware of this in modelling


# Scatter plot of loan amount vs. interest rate
# The vertical lines of points will correspond to the popular loan amounts
ggplot(data_filtered, aes(x = loan_amnt, y = int_rate)) +
  geom_point(alpha = 0.3) + # sets transparency of points, which helps when they overlap
  #geom_smooth(method = "loess", color = "red") + # Add a trend line
  labs(title = "Interest Rate vs. Loan Amount", x = "Loan Amount", y = "Interest Rate") +
  theme_minimal()
```
```{r}
# loan amnt in log scale for visualisation
# 1. Create a new log-transformed variable
data_filtered <- data_filtered %>%
  mutate(log_loan_amnt = log(loan_amnt))

# 2. Re-plot the relationship using the transformed variable
ggplot(data_filtered, aes(x = log_loan_amnt, y = int_rate)) +
  geom_point(alpha = 0.3) +
#  geom_smooth(method = "lm", color = "red") + # Using 'lm' (linear model) now, as the log transformation should linearize the relationship
  labs(title = "Interest Rate vs. Log(Loan Amount)", x = "Log(Loan Amount)", y = "Interest Rate") +
  theme_minimal()


# Not very insightful, but we can explain the spike. We preserve the data as is for interpretability, capturing the max limit. Yet the con is the modeling challenge
# Non-linearity: model will struggle to fit
```

```{r}
# Annual income

# Check minimum income and count of zero/negative incomes
data_filtered %>%
  summarise(
    min_income = min(annual_inc, na.rm = TRUE),
    negative_count = sum(annual_inc <= 0, na.rm = TRUE)
  ) #There are 2 rows with income <= 0, if zero potential realistic, need to look at in detail

# Show entire row where annual_inc == 0
zero_income_row <- data_filtered %>%
  filter(annual_inc == 0)
print(zero_income_row)


# Sort the data by annual_inc in descending order and inspect the top 20 rows
top_income_borrowers <- data_filtered %>%
  # Arrange by annual_inc from highest to lowest
  arrange(desc(annual_inc)) %>%
  # Select key variables for context
  select(loan_amnt, annual_inc, int_rate, term, purpose) %>%
  head(50)

print(top_income_borrowers)

```
There are 2 rows with borrowers with zero income. We do not want to drop these rows, since they have valuable information of this small subgroup.
Since the dti is 999 (because we divide by zero). we should change it to NA and impute it later
```{r}
# where annual_inc == 0, set dti to NA
data_filtered <- data_filtered %>%
  mutate(dti = ifelse(annual_inc == 0, NA, dti))

```

The distribution of annual_inc exhibits extreme right-skewness and numerous high-value outliers. After investigation, we conclude that these high values should be retained but transformed.
1. Validity Assessment: Why Outliers Are Retained
We consider the extreme high-income values (e.g., >$7 million) as valid data points representing a genuine segment of the population, not data entry errors.
Consistency Check: These high incomes are consistently associated with the lowest interest rates (int_rate Grade A) and near-zero Debt-to-Income ratios (dti).

Conclusion: This strong correlation with the lowest-risk loan profile confirms that the lender assessed these individuals as low-risk borrowers, validating the high income figures as genuine extreme outliers representing the top wealth percentile. Dropping them would remove a key, distinct borrower segment from the model's training data.

2. Data Transformation: Log-Transforming annual_inc
A log transformation is required to mitigate the disproportionate influence of these extreme outliers on the predictive model.
Problem: The extreme right-skewness and scale of the data violates assumptions of linearity and normality central to many regression techniques.
Solution: Applying a logarithm (ln) compresses the scale (minimizing the difference between, say, $1M and $10M) and linearizes the relationship between income and the target variable, int_rate.
This transformation makes the resulting log_annual_inc variable much more suitable for standard regression modeling.

```{r}
# Assuming 'annual_inc_clean' is the variable after imputing 0s with the median
data_filtered <- data_filtered %>%
  mutate(log_annual_inc = log(annual_inc))

# Re-check the distribution using the custom function
plot_numeric_distributions(data = data_filtered, target_variable = "int_rate")
```

# Dti (debt to income ratio)
```{r}
# look at the lowest dti values
data_filtered %>%
  arrange(dti) %>%
  select(loan_amnt, annual_inc, dti, int_rate, term, purpose) %>%
  head(20)

# debt_consolidation for dti == 0 is strange


# look at the highest dti values
data_filtered %>%
  arrange(desc(dti)) %>%
  select(loan_amnt, annual_inc, dti, int_rate, term, purpose) %>%
  head(50)

```
DTI >> 100, especially those over 200, are highly improbable as valid credit metrics for approved loans and should be treated as errors for modeling purposes.
The int_rate are not extremely high, so either we set these to NA and impute, or we drop these few rows. Espacially the last 3 rows with dti > 300


DTI == 0 but purpose is debt_consolidation is strange. Search for this combination. Either the dti or purpose is wrong, or the debt was only recently incurred. 
```{r}
# Find rows with dti == 0 and purpose == 'debt_consolidation'
suspicious_dti_rows <- data_filtered %>%
  filter(dti == 0 & purpose == 'debt_consolidation')
print(suspicious_dti_rows)
```
Probably drop or set dti to NA and impute later

```{r}
#Make histogram for dti between 1 and 100
plot_numeric_distributions(data = data_filtered %>% filter(dti >= 0 & dti <= 100), 
                           target_variable = "int_rate", 
                           bins = 50)
```


# Delinq
```{r}
# Look for delinq_2years > 0 AND mths_since_last_delinq >24
suspicious_delinq_rows <- data_filtered %>%
  filter(delinq_2yrs > 0 & mths_since_last_delinq > 24)
print(suspicious_delinq_rows)

# Here either the delinq_2yrs or mths_since_last_delinq is wrong. 
# Prioritize the most direct risk metric and impute the inconsistent value.
# We assume the delinq_2_yrs count is correct. The metric is a direct count of recent risk events and is often a primary feature n lending models. Therefore, we use it to correct the less reliable time-since-event column.
# For every row where the contradiction exists, the value in mths_since_last_delinq will be set to NA for imputation later.

data_filtered <- data_filtered %>%
  mutate(mths_since_last_delinq = ifelse(delinq_2yrs > 0 & mths_since_last_delinq > 24, 
                                         NA, 
                                         mths_since_last_delinq))
```
```{r}
# show lowest values of delinq_2 years
data_filtered %>%
  arrange(delinq_2yrs) %>%
  select(loan_amnt, delinq_2yrs, mths_since_last_delinq, int_rate, term, purpose) %>%
  head(20)
# show lowest values of mths_since_last_delinq
data_filtered %>%
  arrange(mths_since_last_delinq) %>%
  select(loan_amnt, delinq_2yrs, mths_since_last_delinq, int_rate, term, purpose) %>%
  head(20)
# show highest values of mths_since_last_delinq
data_filtered %>%
  arrange(desc(mths_since_last_delinq)) %>%
  select(loan_amnt, delinq_2yrs, mths_since_last_delinq, int_rate, term, purpose) %>%
  head(20)
# show highest values of delinq_2 years
data_filtered %>%
  arrange(desc(delinq_2yrs)) %>%
  select(loan_amnt, delinq_2yrs, mths_since_last_delinq, int_rate, term, purpose) %>%
  head(20)
```
```{r}
# Search for mths_since_last_delinq == 0 and delinq_2yrs == 0 -> contradictory
suspicious_delinq_zero_rows <- data_filtered %>%
  filter(mths_since_last_delinq == 0 & delinq_2yrs == 0)
print(suspicious_delinq_zero_rows)
```
The reason the priority flips isn't about the variable name, but about the specificity and reliability of the value within that variable for the given contradictory context.
1. Case 1: Vague Value (mths_since_last_delinq>24)
When mths_since_last_delinq is vague (any number greater than 24, like 30, 48, or 100):
Trust delinq_2yrs>0 (The Count): The count is the clearest statement that an event did happen recently. The time value >24 is ambiguous and could easily be a placeholder, a miscalculation, or a historical entry that wasn't properly nulled out when the new delinquency occurred.

2. Case 2: Highly Specific Value (mths_since_last_delinq=0)
When mths_since_last_delinq is highly specific (0):
Trust mths_since_last_delinq=0 (The Specific Time): A zero value means the event happened in the current billing cycle. This is a precise, high-granularity data point. The delinq_2yrs=0 is almost certainly an error where the generic, summary count failed to update immediately after the specific event was recorded.

```{r}
# For rows where mths_since_last_delinq == 0 and delinq_2yrs == 0, set delinq_2yrs to 1
data_filtered <- data_filtered %>%
  mutate(delinq_2yrs = ifelse(mths_since_last_delinq == 0 & delinq_2yrs == 0, 
                              1, 
                              delinq_2yrs))
```

```{r}
# Search for mths_since_last_delinq > 24 and acc_now_delinq > 0
suspicious_acc_now_delinq_rows <- data_filtered %>%
  filter(mths_since_last_delinq > 24 & acc_now_delinq > 0)
print(suspicious_acc_now_delinq_rows)


# Search for delinq_2yrs == 0 and acc_now_delinq > 0
suspicious_acc_now_delinq_rows_2 <- data_filtered %>%
  filter(delinq_2yrs == 0 & acc_now_delinq > 0)
print(suspicious_acc_now_delinq_rows_2)
```


```{r}
# Check distributions again
plot_numeric_distributions(data = data_filtered, target_variable = "int_rate")
```
Possibly change delinq_2yrs into a factor for regression models



# Inq_last_6mths
```{r}
# Show highest and lowest values of inq_last_6mths
data_filtered %>%
  arrange(inq_last_6mths) %>%
  select(loan_amnt, inq_last_6mths, int_rate, term
) %>%
  head(20)
data_filtered %>%
  arrange(desc(inq_last_6mths)) %>%
  select(loan_amnt, inq_last_6mths, int_rate, term
) %>%
  head(20)
```
The distribution of the data looks okay, no negative values and no extremes

# Mths since last record and pub_record
If months > 0 then pub_record has to be >= 1
If months = 0 then pub_record has to be >= 1 (one if it is in the same month)
If there is an NA value in months then the customer never had a public record, so pub_record has to be 0

```{r}
# Search for rows with months > 0 and pub_record = 0 or NA
suspicious_pub_record_rows <- data_filtered %>%
  filter((mths_since_last_record > 0 & (is.na(pub_rec) | pub_rec == 0)
  ))
print(suspicious_pub_record_rows) # 0 rows found


# Search for rows with months = 0 and pub_record = NA
suspicious_pub_record_rows_2 <- data_filtered %>%
  filter((mths_since_last_record == 0 & is.na(pub_rec)
  ))
print(suspicious_pub_record_rows_2) # 0 rows found


# Search for rows with months = 0 and pub_record > 0
suspicious_pub_record_rows_3 <- data_filtered %>%
  filter((mths_since_last_record == 0 & pub_rec == 0
  ))
print(suspicious_pub_record_rows_3) # 8 rows found.    -- for pub_rec == 0 we have 1'136 hits. Question our logic, since this would mean that 1'136 rows are wrong and should be changed to NA to be imputed later (or dropped entirely)

# Check logic, search how many rows mths_since_last_record = 0 and pub_record >= 1 (should have many hits)
suspicious_pub_record_rows_4 <- data_filtered %>%
  filter((mths_since_last_record == 0 & pub_rec >= 1
  ))
print(suspicious_pub_record_rows_4) # 8 hits

# Search for rows where mths_since_last_record is NA, pub_record has to be here NA or 0, so find all which are > 0
suspicious_pub_record_rows_5 <- data_filtered %>%
  filter(is.na(mths_since_last_record) & pub_rec > 0
  )
print(suspicious_pub_record_rows_5) # 0 rows found



```
Drop rows where both mths_since_last_record and pub_rec are NA --> you can't be sure if it is missing, or if the customer never had a public record
For those in mths_since_last record NA and pub_rec == 0 we keep it as is --> change to factorial and level 'none' later
Impute all NA of pub_record where mths_since_last_record != NA

# Check distribution of values of mths_since_last_record and pub_record to find strange values
```{r}
# lowest values of mths_since_last_record
data_filtered %>%
  arrange(mths_since_last_record) %>%
  select(loan_amnt, mths_since_last_record, pub_rec, int_rate, term) %>%
  head(20)
# highest values of mths_since_last_record
data_filtered %>%
  arrange(desc(mths_since_last_record)) %>%
  select(loan_amnt, mths_since_last_record, pub_rec, int_rate, term) %>%
  head(20)
# lowest values of pub_rec
data_filtered %>% 
  arrange(pub_rec) %>%
  select(loan_amnt, mths_since_last_record, pub_rec, int_rate, term) %>%
  head(20)
# highest values of pub_rec
data_filtered %>% 
  arrange(desc(pub_rec)) %>%
  select(loan_amnt, mths_since_last_record, pub_rec, int_rate, term) %>%
  head(20)

```


# open_acc
open_acc must be less or equal to total_acc (total_acc is the count of all accounts ever openend). Check for rows where open_acc > total_acc
```{r}
# Search for rows with open_acc > total_acc
suspicious_open_acc_rows <- data_filtered %>%
  filter(open_acc > total_acc)
print(suspicious_open_acc_rows) # 2 rows found
```
You should not change total_acc to the value of open_acc. Changing the total_acc value risks fabricating historical data that might be wrong.
The issue is that open_acc is impossible given the total_acc count. Therefore, the open_acc value is the one that should be corrected.
Recommended Correction: Convert open_acc to NA
For only 2 rows, the safest and most robust action is to set open_acc to NA and let your general imputation strategy handle the final value.

# Check NA of open_acc
```{r}
na_open_acc_count <- sum(is.na(data_filtered$open_acc))
print(paste("Number of NA in open_acc:", na_open_acc_count))
```

# Check NA of total_acc
```{r}
na_total_acc_count <- sum(is.na(data_filtered$total_acc))
print(paste("Number of NA in total_acc:", na_total_acc_count))
```

# Number of accounts where open_acc AND total_acc are both NA
```{r}
na_both_count <- sum(is.na(data_filtered$open_acc) & is.na(data_filtered$total_acc))
print(paste("Number of rows with both open_acc and total_acc as NA:", na_both_count))
```
It is best to dropthese 25 rows --> not a lot in the entire dataset and since the correlation of one to the other is missing, it is hard to impute them logically

# Find lowest and highest values of open_acc and total_acc
```{r}
# lowest values of open_acc
data_filtered %>%
  arrange(open_acc) %>%
  select(loan_amnt, open_acc, total_acc, int_rate, term) %>%
  head(20)
# highest values of open_acc
data_filtered %>%
  arrange(desc(open_acc)) %>%
  select(loan_amnt, open_acc, total_acc, int_rate, term) %>%
  head(20)
# lowest values of total_acc
data_filtered %>% 
  arrange(total_acc) %>%
  select(loan_amnt, open_acc, total_acc, int_rate, term) %>%
  head(20)
# highest values of total_acc
data_filtered %>% 
  arrange(desc(total_acc)) %>%
  select(loan_amnt, open_acc, total_acc, int_rate, term) %>%
  head(20)
```


Interpretation of open_acc and total_acc
A high total_acc indicates a long credit history, while a high open_acc indicates the borrower is managing a high number of active lines of credit, which can be a higher risk signal.

# Revol_bal
This is the revolving balance. there are a few other features to look at to check data integrity
total_rev_hi_lim (the maximum available credit) should logically always be bigger or equal to revol_bal: check for rows where revol_bal > total_rev_hi_lim
revol_util is a ratio calculated by revol_bal/total_rev_hi_lim (potentially * 100). Check for rows where revol_util * total_rev_hi_lim < revol_bal (if revol_util is in percentage, then revol_util/100 * total_rev_hi_lim < revol_bal)
If there is a balance, then an account has to be open. Therefore if revol_balf > 0 then open_acc has to be > 0 (subsequently total_acc has to be > 0 as well)

```{r}
# Search for rows with revol_bal > total_rev_hi_lim
suspicious_revol_bal_rows <- data_filtered %>%
  filter(revol_bal > total_rev_hi_lim)
print(suspicious_revol_bal_rows) # 3'269 rows found

# Search for rows with revol_util * total_rev_hi_lim < revol_bal
suspicious_revol_util_rows <- data_filtered %>%
  filter(revol_util / 100 * total_rev_hi_lim < revol_bal)
print(suspicious_revol_util_rows) # 367'136 rows found

# Search for rows with revol_bal > 0 and open_acc == 0
suspicious_revol_bal_open_acc_rows <- data_filtered %>%
  filter(revol_bal > 0 & open_acc == 0)
print(suspicious_revol_bal_open_acc_rows) # 0 rows found
```

For case one: 
total_rev_hi_lim represents the absolute, legally defined maximum amount of credit a borrower has available across all their revolving accounts. This value can only increase if a lender officially raises the limit, or if a new line of credit is opened.
Logical Impossibility: The revol_bal (Revolving Balance) is the amount owed. If the amount owed exceeds the total available limit, the revol_bal figure must be wrong because it violates the hard ceiling of the credit system. Even if the limit was slightly delayed in updating, 
balance exceeding the limit by a large amount points to a data corruption error in the balance, not the limit.

The safest action is to set the faulty revol_bal to NA and let your imputation strategy handle it later.


For case two:
For the 367,136 rows, the discrepancy is so pervasive that cleaning every one is difficult. The most powerful variable among the three is the Utilization Rate (revol_util), as it is the metric lenders focus on for risk.

The best approach is to re-calculate revol_util based on the cleaned revol_bal and total_rev_hi_lim columns.






# Check for minus values and fillers like 999 
# Ensure Dtypes are correct
# Make Histograms / Density plot (numeric)
# Make barplots (categorical)
# Make Boxplots
# Identify outliers
# Identify imputed values / wrong values
# Bivariate exploration: Scatterplots: numeric vs numeric | Boxplots: numeric vs categorical | Contingency tables: categorical vs categorical | Correlation matrix (very helpful)


# Change variables like pub_rec to factorial in linear regression, the risk and so change in int_rate is more likely to change from pub_rec == 0 or 1, and less risk change from e.g. 45 -> 50
view(data_filtered)


