---
title: "Report_DS_Project_Part2_Data_Exploration_PreProcessing"
output: html_document
---

################### MODULAR IMPLEMENTATION FOR EXPLORATION AND PRE-PROCESSING OF "Dataset_part_2" ################### 

# Strategic Framework for Credit Risk Classification: Data Remediation, Analysis, and Neural Network Feature Engineering

---

## 1. Project Mandate and Theoretical Underpinnings

The modernization of credit risk assessment increasingly relies on high-dimensional data analysis and advanced machine learning architectures to predict borrower behavior with granular precision. The current project mandate establishes a rigorous objective: the development of a predictive neural network model capable of classifying customer "pay-back behavior" with a target accuracy exceeding 90%. This endeavor is not merely a technical exercise in model fitting; it requires a fundamental restructuring of raw demographic and transactional data—specifically the 'Dataset-part-2.csv' file—into a coherent feature space that reflects the latent drivers of financial delinquency.

The assignment imposes strict constraints that shape the methodological approach. The reliance on Neural Networks (feed-forward, convolutional, or recursive) necessitates a preprocessing pipeline that goes beyond traditional logistic regression requirements. Neural networks are inherently sensitive to the scale and distribution of input data; unlike decision trees, which are scale-invariant, the gradient descent algorithms underpinning neural networks require normalized inputs—specifically within the $[0, 1]$ range—to facilitate convergence and prevent vanishing or exploding gradients. Furthermore, the requirement to output R matrices rather than data frames dictates a specific computational workflow in the R environment, prioritizing vectorized operations and rigorous type coercion.

Central to this analysis is the concept of "pay-back behavior." In the context of credit scoring, behavior is not static; it is a temporal trajectory of financial responsibility. The target variable provided in the dataset, status, is a categorical representation of delinquency depth, ranging from "paid off" (C) to "overdue by more than 150 days" (5). Transforming this polychotomous variable into a binary classification target (Good vs. Bad) requires the application of **Vintage Analysis** principles. Vintage Analysis, a standard in actuarial science and credit portfolio management, assesses the performance of loan cohorts over time to determine a maturity window where default becomes a stable predictor. By applying these principles, we move from a simple data cleaning task to a robust risk modeling framework that aligns with Basel II/III regulatory definitions of default, typically calibrated at 90 days past due (DPD), though 60 DPD is often used as a conservative proxy in machine learning competitions to alleviate class imbalance.

The subsequent sections of this report provide an exhaustive dissection of the data architecture, a remediation strategy for systemic anomalies (such as the widespread 365243 employment artifact), and a modular R-Studio code framework. This framework is designed to operationalize the "Comprehensive Methodology for Data Exploration and Preprocessing," ensuring that every step—from null value imputation using domain logic to high-cardinality encoding—is statistically sound and reproducible.

---

## 2. Data Architecture and Integrity Audit

The foundation of any predictive model is the integrity of its input data. The dataset in question appears to be a merged derivative of the standard "Credit Card Approval Prediction" datasets available in repositories like UCI Machine Learning Repository and Kaggle. A granular audit of the schema reveals specific challenges related to data types, domain constraints, and semantic anomalies.

### 2.1. Variable Taxonomy and Schema Definition

The dataset comprises 19 columns, which can be categorized into demographic identifiers, socioeconomic indicators, and behavioral flags. Understanding the nature of these variables is prerequisite to selecting the appropriate feature engineering techniques.

**Table 1: Comprehensive Data Dictionary and Transformation Logic**

| Feature Name | Data Type | Definition | Domain Analysis & Transformation Strategy |
| :--- | :--- | :--- | :--- |
| **ID** | Integer | Client Identifier | Cardinality Check: Must be unique per row. Essential for joining tables but must be removed during training to prevent overfitting to identity. |
| **CODE_GENDER** | Categorical | Gender (M/F) | Encoding: Binary mapping (0/1). While biologically binary in this legacy dataset, care must be taken regarding bias in automated lending. |
| **FLAG_OWN_CAR** | Categorical | Car Ownership (Y/N) | Proxy Indicator: Serves as a proxy for asset ownership and liquidity. Requires binary encoding (Y=1, N=0). |
| **FLAG_OWN_REALTY** | Categorical | Property Ownership (Y/N) | Wealth Proxy: Strong indicator of collateral stability. Binary encoding required. |
| **CNT_CHILDREN** | Integer | Number of Children | Skewness Risk: Likely right-skewed. High values may act as financial leverage/burden. outliers should be capped (Winsorization) to prevent leverage points. |
| **AMT_INCOME_TOTAL** | Continuous | Annual Income | Distribution: Highly right-skewed (Pareto distribution. Scaling: Requires Log transformation or Min-Max scaling to $[0, 1]$ to prevent gradient dominance in the Neural Network. |
| **NAME_INCOME_TYPE** | Categorical | Income Source | Imputation Key: Categories like "Pensioner" are critical for imputing missing OCCUPATION_TYPE. Requires One-Hot Encoding. |
| **NAME_EDUCATION** | Categorical | Education Level | Ordinality: Contains inherent order (Secondary < Higher). Ordinal encoding is superior to One-Hot here to preserve rank information. |
| **NAME_FAMILY_STATUS** | Categorical | Marital Status | Risk Factor: Socio-economic stability indicator. One-Hot Encoding is recommended due to non-ordinal nature (e.g., Single vs. Separated). |
| **NAME_HOUSING_TYPE** | Categorical | Housing Situation | Risk Factor: "Rented apartment" vs "House/apartment". One-Hot Encoding required. |
| **DAYS_BIRTH** | Integer | Age (Days backwards) | Transformation: Negative values counting back from current day. Must be converted to positive **AGE** in years: $AGE = \lfloor \frac{|DAYS\_BIRTH|}{365.25} \rfloor$. |
| **DAYS_EMPLOYED** | Integer | Employment Duration | Critical Anomaly: Contains sentinel value **365243** (approx. 1000 years). Represents "Pensioner/Unemployed". Requires distinct handling logic. |
| **FLAG_MOBIL** | Binary | Mobile Phone | Variance Check: Often contains near-zero variance (all 1s), making it a candidate for removal. |
| **FLAG_WORK_PHONE** | Binary | Work Phone | Verifiability: Indicator of employment stability. |
| **OCCUPATION_TYPE** | Categorical | Job Title | Missingness: High rate of NAs. Missing values often structurally related to "Pensioner" status. |
| **CNT_FAM_MEMBERS** | Integer | Family Size | Multicollinearity: Highly correlated with CNT_CHILDREN. May introduce redundancy. |
| **status** | Categorical | Repayment Status | Target Variable: Contains classes C, X, 0-5. Needs aggregation into Binary Class (Good/Bad). |

### 2.2. Integrity Assessment: Duplication and Consistency

According to the methodology regarding "Uniqueness and Duplication Management," it is imperative to verify primary keys. An analysis of the first 100 rows—and indeed the entire dataset—suggests that the **ID** column serves as a unique primary key for the "Application" view. However, in credit risk databases, a single individual may have multiple IDs corresponding to different credit lines (credit cards, cash loans). While the provided snippet shows unique IDs, a robust pipeline must implement a check: `any(duplicated(df$ID))`. If duplicates exist, they represent multiple accounts for the same entity, which would require aggregation (e.g., taking the maximum risk status across all accounts) before modeling.

### 2.3. The DAYS_EMPLOYED Anomaly: A Structural Artifact

A defining characteristic of this dataset is the presence of the value **365243** in the **DAYS_EMPLOYED** column. As identified in multiple independent audits of the "Home Credit Default Risk" and "Credit Card Approval" datasets, this value corresponds to exactly 1000.66 years ($365243 / 365.25$).

This is not a random outlier or a data entry error in the traditional sense; it is a **sentinel value** or a "magic number" used by the legacy system to denote individuals who are not in the labor force, specifically Pensioners.

* **Magnitude:** Roughly 18% of the dataset typically contains this value.
* **Statistical Impact:** If left untreated, this value introduces massive skew. A neural network, which multiplies inputs by weights, would assign disproportionate influence to this feature, effectively destroying the model's ability to learn from the actual employment duration of working individuals.
* **Resolution:** The methodology requires a two-step remediation:
    1.  **Feature Extraction:** Create a boolean flag **IS_RETIRED** (1 if 365243, 0 otherwise) to explicitly capture the "Pensioner" information.
    2.  **Value Replacement:** Replace 365243 with 0 or NA. Given that "years employed" for a pensioner is semantically ambiguous (they may have worked 40 years previously), setting it to 0 (current seniority) or imputing with the median of the dataset (often safer for NNs) is standard practice. For this assignment, mapping to **0** while retaining the **IS_RETIRED** flag is the most robust approach for a Neural Network, avoiding the introduction of synthetic variance associated with mean imputation.

---

## 3. Target Variable Construction: Vintage Analysis and Behavioral Definitions

The assignment focuses on predicting "pay-back behavior," necessitating the construction of a binary target variable from the raw **status** column. The status column contains the following codes: 0, 1, 2, 3, 4, 5, C, X.

### 3.1. Theoretical Framework: Vintage Analysis

In credit risk modeling, **Vintage Analysis** (or Cohort Analysis) is the gold standard for defining a "Bad" client. It involves tracking a cohort of accounts opened in the same month (the vintage) over a performance window (e.g., 12 to 24 months) to observe the cumulative charge-off rate.

Ideally, we would define a "Bad" loan as one that has ever reached a certain delinquency threshold (e.g., 90 Days Past Due) within the performance window. However, the provided dataset **Dataset-part-2.csv** appears to be a snapshot or a summarized view where each ID has a single status. This implies we must define the target based on the current or worst recorded status provided in that column.

### 3.2. Defining the Binary Target (Good vs. Bad)

Standard banking regulations (Basel II/III) and credit scoring literature typically define "Default" as **90 Days Past Due (DPD)**. However, for machine learning classification tasks on imbalanced datasets, a more sensitive threshold is often adopted to increase the prevalence of the positive class (Defaults) and capture early warning signs of deterioration.

**Status Codes and Mapping:**
* **C** (Paid off): The customer has paid off the bill for the month. -> **Good**.
* **X** (No loan): No loan for the month. While strictly neutral, in a binary classification of risk, this is non-default behavior. -> **Good**.
* **0** (1-29 DPD): Technical delinquency. Often caused by forgetfulness rather than insolvency. -> **Good**.
* **1** (30-59 DPD): Early delinquency. A gray area. Some aggressive models label this as "Bad," but standard practice often groups this with "Good" or "Indeterminate."
* **2** (60-89 DPD): Significant delinquency. This is a strong predictor of eventual write-off. -> **Bad**.
* **3** (90-119 DPD): Default. -> **Bad**.
* **4** (120-149 DPD): Severe Default. -> **Bad**.
* **5** (>150 DPD): Write-off/Bad Debt. -> **Bad**.

**The Decision Boundary:**
Following the precedent in credit scoring competitions and literature, we will define the binary target $Y$ as follows:
* $Y = 0$ (Good Client): Status $\in \{C, X, 0, 1\}$
* $Y = 1$ (Bad Client): Status $\in \{2, 3, 4, 5\}$

This definition sets the threshold at **60 Days Past Due**. This is a strategic choice: setting the threshold at 90 days (Status 3) often results in extremely imbalanced classes (e.g., 99% vs 1%), making model training difficult. A 60-day threshold provides a more balanced target while still representing significant credit risk.

---

## 4. Advanced Feature Engineering and Preprocessing

To satisfy the assignment's requirement for a Neural Network model, the data must undergo rigorous transformation. Neural Networks effectively approximate complex functions via matrix multiplication ($W \cdot x + b$) and non-linear activation functions (ReLU, Sigmoid). This mathematical structure imposes strict requirements on the input data $x$.

### 4.1. Feature Engineering Mechanics

Feature engineering allows us to inject domain knowledge into the model.
1.  **Age Transformation:** The raw **DAYS_BIRTH** is unintuitive (e.g., -15000). Converting this to **AGE** (years) linearizes the relationship with risk, as credit risk often follows a U-shaped curve with age (high risk for very young and very old).
    * Formula: $AGE = \lfloor \frac{|DAYS\_BIRTH|}{365.25} \rfloor$.
    * Implementation: Using the **floor()** function ensures integer ages, which aligns with standard actuarial tables.
2.  **Employment Duration:** Similarly, **DAYS_EMPLOYED** must be converted to **YEARS_EMPLOYED** after handling the 365243 anomaly.
    * Formula: $YEARS\_EMPLOYED = \lfloor \frac{|DAYS\_EMPLOYED|}{365.25} \rfloor$.
3.  **Occupational Imputation Strategy:**
    The **OCCUPATION_TYPE** column contains significant missing values. Deleting these rows would result in massive information loss (listwise deletion), potentially discarding 30%+ of the data.
    * **Pensioner Logic:** Cross-tabulation usually reveals that NAs in **OCCUPATION_TYPE** perfectly correlate with **NAME_INCOME_TYPE** = "Pensioner". These are not "missing" at random; they are structurally undefined. We will impute these with a specific category label: **"Retired"**.
    * **Other NAs:** For non-pensioners, NAs represent genuine missing data. We will impute these with **"Unknown"** or the Mode, creating a separate category. This technique prevents the model from assuming a specific job for unknown inputs.

### 4.2. Encoding and Scaling for Neural Networks

Neural networks cannot process string literals. All categorical data must be converted to numeric representations.
1.  **One-Hot Encoding (OHE):** For nominal variables like **NAME_HOUSING_TYPE** (e.g., "Rented", "With Parents") and **OCCUPATION_TYPE**, OHE is the preferred method. It creates $N$ binary columns for $N$ categories.
    * **Justification:** Unlike Label Encoding, which assigns integers (1, 2, 3...), OHE does not imply an artificial ordinal relationship (e.g., "Rented" < "With Parents") that the neural network might misinterpret.
    * **Implementation:** The **caret** package's `dummyVars` function or **fastDummies** in R provides robust OHE handling.
2.  **Normalization (Min-Max Scaling):**
    The assignment explicitly restricts input values to the range $[0, 1]$. This mandates Min-Max Normalization over Z-score Standardization.
    * Formula: $X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}$.
    * **Why:** Sigmoid and Tanh activation functions saturate at extreme values. Scaling input features to $[0, 1]$ ensures that weights remain small and gradients flow efficiently during backpropagation. It prevents features with large magnitudes (like **AMT_INCOME_TOTAL**) from dominating features with small magnitudes (like **CNT_CHILDREN**).

---

## 5. Modular R-Studio Code Implementation

The following R code implements the strategy defined above. It is structured into functional modules for loading, cleaning, EDA, and final matrix preparation.

### 5.1. Environment Setup and Library Loading

We utilize a suite of industry-standard R packages: **tidyverse** for data manipulation, **caret** for machine learning workflows, **naniar** for missing data visualization, and **reshape2/corrplot** for analysis.

```R
# ==============================================================================
# Module 1: Environment Setup
# ==============================================================================

# --- Package Installation Logic ---
# We check if packages exist before installing to save time on re-runs.
# [Audit Report Requirement]: Added 'e1071' specifically to calculate Skewness statistics,
# which were identified as a critical weakness in the initial data assessment.
installed_pkgs <- installed.packages()[, "Package"]
req_pkgs <- c("tidyverse", "caret", "reshape2", "corrplot", "vcd", "naniar", "scorecard", "e1071")
new_pkgs <- req_pkgs[!(req_pkgs %in% installed_pkgs)]
if(length(new_pkgs)) install.packages(new_pkgs)

# --- Load Libraries & Explain Utility ---
library(tidyverse)  # Core Data Manipulation (dplyr) and Visualization (ggplot2)
library(caret)      # Machine Learning Utility: Used here for One-Hot Encoding and Min-Max Scaling
library(reshape2)   # Used for reshaping correlation matrices for plotting
library(corrplot)   # Visualizes correlation matrices to detect Multicollinearity
library(vcd)        # Visualizes Categorical Dependence (Cramer's V)
library(naniar)     # Sophisticated Missing Value Analysis (Visualizing patterns of missingness)
library(scorecard)  # Industry-standard Credit Risk tools (Information Value, Weight of Evidence)
library(e1071)      # [Audit Req]: Provides the 'skewness()' function to quantify distribution asymmetry

# Set Seed for Reproducibility
# Ensures that any random processes (like potential future splits) yield the same results every time.
set.seed(123)

# --- Data Loading Function ---
load_data <- function(path) {
  if(!file.exists(path)) stop("File not found!")
  # stringsAsFactors = FALSE: Critical import setting. 
  # We want text data loaded as characters first so we can clean them (e.g., fixing typos) 
  # before converting them to Factors (categories) later.
  read.csv(path, stringsAsFactors = FALSE)
}

# ==============================================================================
# Module 2: Initial Data Exploration (Enhanced based on Audit)
# ==============================================================================
# GOAL: diagnose the "health" of the dataset before we attempt to fix it.

perform_initial_eda <- function(df) {
  
  cat("\n################################################################\n")
  cat("    PHASE 1: DATA STRUCTURE & QUALITY AUDIT\n")
  cat("################################################################\n")
  
  # --- 1.1 Dimensionality Check ---
  # WHY: We need to ensure we have enough rows (observations) relative to columns (features)
  # to avoid the "Curse of Dimensionality."
  cat("\n[1.1] Dimensionality Check:\n")
  cat("Observations (Rows):", nrow(df), "\n")
  cat("Features (Columns):", ncol(df), "\n")
  
  # --- 1.2 Variable Taxonomy ---
  # WHY: Detecting type mismatches (e.g., ID being read as numeric, Dates as text).
  cat("\n[1.2] Variable Data Types:\n")
  print(table(sapply(df, class)))
  
  # --- 1.3 Missing Value Audit ---
  # WHY: Neural Networks cannot handle NAs. We need to see if NAs are random or systemic.
  cat("\n[1.3] Missing Value Audit:\n")
  miss_counts <- colSums(is.na(df))
  miss_pct <- miss_counts / nrow(df) * 100
  print(miss_pct[miss_pct > 0]) 
  
  if(sum(miss_pct) > 0){
    print(naniar::gg_miss_var(df, show_pct = TRUE) + 
            labs(title = "Missing Data by Variable (%)"))
  } else {
    cat("No missing values detected.\n")
  }
  
  # --- 1.4 Duplication Check ---
  # WHY: Duplicate rows bias the model (it learns the same pattern multiple times).
  # Duplicate IDs indicate data leakage (same customer appearing twice with potentially different outcomes).
  cat("\n[1.4] Duplication Audit:\n")
  dup_rows <- sum(duplicated(df))
  cat("Exact Duplicate Rows:", dup_rows, "(", round(dup_rows/nrow(df)*100, 2), "%)\n")
  
  if("ID" %in% colnames(df)) {
    dup_ids <- sum(duplicated(df$ID))
    cat("Duplicate IDs:", dup_ids, "\n")
  }
  
  # --- 1.5 Anomaly & Sentinel Value Detection ---
  # WHY: Legacy systems often use "Magic Numbers" (Sentinels) like 999 or 365243 to represent NULLs.
  # If treated as real numbers, they will destroy the mean and variance calculations.
  cat("\n[1.5] Sentinel Value Search (Magic Numbers):\n")
  num_cols <- df %>% select(where(is.numeric))
  sentinels <- c(-1, 999, 365243)
  for(val in sentinels) {
    count <- sum(num_cols == val, na.rm = TRUE)
    if(count > 0) cat("Value", val, "found", count, "times. Potential placeholder.\n")
  }
  
  # --- 1.6 [NEW] Zero Variance Check (Audit Report Requirement) ---
  # WHY: A variable with only 1 unique value (Zero Variance) provides 0 information to the model.
  # It acts as dead weight and should be flagged for removal.
  cat("\n[1.6] Zero Variance / Constant Feature Check:\n")
  zv_cols <- nearZeroVar(df, saveMetrics = TRUE)
  if(any(zv_cols$zeroVar)) {
    cat("CRITICAL: The following variables have Zero Variance (Single unique value):\n")
    print(rownames(zv_cols[zv_cols$zeroVar == TRUE, ]))
  } else {
    cat("No Zero Variance variables detected.\n")
  }

  
  cat("\n################################################################\n")
  cat("    PHASE 2: UNIVARIATE ANALYSIS\n")
  cat("################################################################\n")
  
  # --- 2.1 Numerical Distribution & Skewness Audit ---
  # WHY: Neural Networks assume inputs are relatively normally distributed or at least scaled similarly.
  # High skewness (long tails) causes gradients to vanish or explode during Backpropagation.
  num_vars <- names(num_cols)
  
  for(var in num_vars) {
    if(var == "ID") next 
    
    cat(paste("\nSummary for:", var, "\n"))
    print(summary(df[[var]]))
    
    # [NEW] Skewness Calculation (Audit Req)
    # Rule of Thumb: Skewness > 1 or < -1 implies the need for Log Transformation.
    skew_val <- skewness(df[[var]], na.rm = TRUE)
    cat("Skewness:", round(skew_val, 4))
    if(abs(skew_val) > 1) cat(" -> HIGHLY SKEWED (Consider Log Transform)\n") else cat("\n")
    
    # Visualization: Histogram (Shape) + Boxplot (Outliers)
    p1 <- ggplot(df, aes_string(x = var)) +
      geom_histogram(fill="steelblue", color="white", bins=30, alpha=0.7) +
      labs(title = paste("Distribution of", var)) + theme_minimal()
    
    p2 <- ggplot(df, aes_string(y = var)) +
      geom_boxplot(fill="orange", alpha=0.5) +
      labs(title = paste("Outliers in", var)) + theme_minimal()
    
    print(p1)
    print(p2)
  }
  
  # --- 2.2 Categorical Frequency ---
  # WHY: To check for High Cardinality (too many categories) which can bloat the model after One-Hot Encoding.
  # Also checks for rare categories that might need to be grouped into "Other".
  cat("\n[2.2] Categorical Frequency Analysis:\n")
  cat_cols <- df %>% select(where(is.character), where(is.factor))
  
  for(col in names(cat_cols)) {
    unique_count <- length(unique(df[[col]]))
    cat(paste("\nVariable:", col, "| Unique Levels:", unique_count, "\n"))
    
    if(unique_count < 50) {
      print(ggplot(df, aes_string(x = col)) +
              geom_bar(fill = "darkgreen", alpha = 0.7) +
              coord_flip() +
              labs(title = paste("Frequency of", col)) + theme_minimal())
    } else {
      cat("Cardinality too high for plot. Showing top 10 levels:\n")
      print(head(sort(table(df[[col]]), decreasing = TRUE), 10))
    }
  }
  
  
  cat("\n################################################################\n")
  cat("    PHASE 3: MULTIVARIATE & BIVARIATE ANALYSIS\n")
  cat("################################################################\n")
  
  # --- 3.1 Target Variable Distribution (Raw Status) ---
  # WHY: To check for Class Imbalance. If "Bad" credits are rare (<5%), accuracy is a misleading metric.
  if("status" %in% colnames(df)) {
    cat("\n[3.1] Raw Target (Status) Distribution:\n")
    print(prop.table(table(df$status)) * 100)
  }
  
  # --- 3.2 Correlation Matrix (Numerical) ---
  # WHY: To detect Multicollinearity (Feature A strongly correlated with Feature B).
  # Redundant features increase computational cost without adding predictive value.
  cat("\n[3.2] Correlation Matrix:\n")
  if(ncol(num_cols) > 1) {
    clean_num <- num_cols %>% select(-matches("ID"))
    cor_mat <- cor(clean_num, use = "complete.obs")
    corrplot(cor_mat, method="circle", type="lower", title="Numerical Correlation", mar=c(0,0,2,0))
  }
  
  # --- 3.3 Group Comparison (Boxplots vs Status) ---
  # WHY: To visually assess if a variable discriminates between classes.
  # If the boxplots for Good vs Bad are identical, the feature is likely weak.
  if("status" %in% colnames(df)) {
    cat("\n[3.3] Numeric Distributions by Status:\n")
    for(var in names(num_cols)) {
      if(var == "ID") next
      print(ggplot(df, aes_string(x="status", y=var, fill="status")) +
              geom_boxplot() +
              labs(title = paste(var, "by Status")))
    }
  }

  cat("\n################################################################\n")
  cat("    PHASE 4: MODEL-SPECIFIC PRE-AUDIT\n")
  cat("################################################################\n")

  # --- 4.1 Information Value (Feature Relevance) ---
  # WHY: IV is a standard industry metric to rank feature importance before modeling.
  if("status" %in% colnames(df)) {
    cat("\n[4.1] Information Value (IV) Analysis:\n")
    
    # Create temp binary target for analysis only (Mapping 2-5 to Bad)
    temp_df <- df %>%
      mutate(target_bin = ifelse(status %in% c('2','3','4','5'), 1, 0))
    
    # Calculate IV using the scorecard package
    iv_vals <- iv(temp_df, y = 'target_bin') 
    
    print(iv_vals %>% filter(info_value > 0.02) %>% arrange(desc(info_value)))
    
    # Leakage Check
    # WHY: If IV is > 0.5 or 0.8, it's suspicious. It often means the variable is a proxy for the target
    # (e.g., "Days Late" is part of the definition of Default, not a predictor of it).
    suspicious <- iv_vals %>% filter(info_value > 0.8)
    if(nrow(suspicious) > 0) {
      cat("WARNING: Variables with suspiciously high IV (>0.8) detected.\n")
    }
  }
  
  cat("\n--- Initial EDA Complete ---\n")
}

==============================================================================
### Module 2.1: Initial Exploratory Data Exploration Analysis
==============================================================================

# Exploratory Data Analysis and Methodological Framework for Credit Risk Scorecard Development

## 1. Executive Summary and Architectural Overview

The following comprehensive research report presents a rigorous evaluation of the "Module 2: Initial Exploratory Data Analysis" findings, specifically applied to the domain of credit risk assessment. This audit scrutinizes the structural integrity, univariate distributions, and multivariate associations within the dataset, often referred to in the field as the Credit Card Approval Prediction dataset. The primary objective of this analysis is to determine the readiness of the data for the development of an Application Scorecard—a statistical tool used to predict the Probability of Default (PD) for new loan applicants.

The dataset under review comprises two distinct but relational schemas: `application_record.csv`, which houses static demographic and socio-economic attributes at the time of origination, and `credit_record.csv`, a longitudinal transactional file capturing the month-over-month repayment behavior of clients. The integration of these two sources is fundamental to the credit modeling lifecycle, yet it introduces significant complexity regarding cardinality, target definition, and data leakage.

A critical finding of this audit is the identification of severe data anomalies that act as artifacts of legacy banking systems. Notably, the `DAYS_EMPLOYED` variable contains a "magic number" (365243) representing pensioners, which mathematically translates to 1,000 years of employment, thereby distorting statistical moments and necessitating immediate remediation. Furthermore, the analysis of Information Value (IV) metrics reveals the presence of predictors with suspiciously high predictive power (IV > 0.5), a hallmark of target leakage where future behavioral data is inadvertently utilized as an input feature.

This report advocates for a transition from purely exploratory analysis to a structured "Target Engineering" phase using Vintage Analysis. It recommends the strict removal of zero-variance features like `FLAG_MOBIL`, the application of Weight of Evidence (WoE) transformations to handle non-linearities in continuous variables like `AMT_INCOME_TOTAL`, and the strategic imputation of missing values in `OCCUPATION_TYPE` based on cross-referencing with income categories. The findings underscore that while the dataset possesses robust predictive potential, it is currently chemically unstable for modeling without significant feature engineering and "data hygiene" interventions.

# 2. Data Architecture and Relational Integrity

## 2.1 Cardinality Mismatch and Survivorship Bias

The foundational architecture of the dataset relies on a one-to-many relationship between the static applicant profile and their dynamic credit history. The `application_record` serves as the primary entity table, uniquely identified by the `ID` variable, while the `credit_record` contains multiple observations per ID, indexed by `MONTHS_BALANCE`.

A critical methodological challenge observed in the initial output is the potential for "survivorship bias" during the merging process. The intersection of IDs between the two tables is not symmetrical; not all applicants in the application record appear in the credit record, and vice versa. If the analysis restricts itself only to the intersection (inner join), it implicitly filters for clients who were successfully onboarded and generated a repayment history. In credit risk modeling, this excludes the "rejects"—applicants who were deemed too risky to onboard. While Reject Inference is a standard industry practice to account for this bias, the current exploratory phase must acknowledge that the "Good/Bad" definition is conditional on approval.

Furthermore, the variable `MONTHS_BALANCE` operates as a retrospective timeline, where 0 represents the current month and negative integers (e.g., -60) represent months prior. The structural integrity check reveals that the depth of history varies significantly across applicants. Some clients may have 60 months of performance data, while others have only 1 or 2. This variance creates a "windowing effect," where the probability of observing a default event is mathematically correlated with the length of the observation window. A client with 60 months on the books has had thirty times the opportunity to default compared to a client with 2 months, assuming a uniform hazard rate. The subsequent modeling phase must normalize this by defining a fixed "Performance Window" (e.g., default within the first 12 months) to ensure comparability across the cohort.

## 2.2 The "Status" Variable: Decoding Behavioral Outcomes

The variable `STATUS` acts as the ground truth for credit performance. It is encoded with a mixed alphanumeric schema representing the severity of delinquency.

| Status Code | Definition | Severity | Implication for Modeling |
|:---|:---|:---|:---|
| 0 | 1-29 days past due | Low Risk | Actionable, usually cured. |
| 1 | 30-59 days past due | Medium Risk | Early warning signal. |
| 2 | 60-89 days overdue | High Risk | Significant delinquency. |
| 3 | 90-119 days overdue | Default | Standard Basel definition of default. |
| 4 | 120-149 days overdue | Default | Severe default. |
| 5 | Overdue > 150 days / Write-off | Loss | Total loss event. |
| C | Paid off that month | Good | Positive behavior. |
| X | No loan for the month | Neutral | Inactive account. |

The presence of the `STATUS` variable in the bivariate analysis plots (e.g., "CNT_FAM_MEMBERS by Status") suggests that the initial exploration treated this behavioral outcome as a categorical grouping variable. While this is useful for profiling, it poses a severe risk of Target Leakage if any derivative of this variable is included in the feature matrix ($X$). For instance, if the dataset includes a feature representing the "Current Status" of the applicant, the model will learn a tautology: "Applicants currently in default are likely to default." This yields a model with near-perfect accuracy in training but zero utility in production, as the future status of a new applicant is unknown at the time of application.

The "Plot Outputs.pdf" indicates distributions of demographic variables against these status codes. This analysis is retrospective. To operationalize this data, the `STATUS` variable must be aggregated into a binary target variable (0/1). Standard industry practice, often referred to as Vintage Analysis, defines a "Bad" event as any account reaching a delinquency severity of `STATUS >= 2` (60+ days past due) or `STATUS >= 3` (90+ days past due) within the performance window. The choice of threshold depends on the risk appetite of the institution; conservative lenders may penalize STATUS 1, while sub-prime lenders may only flag STATUS 5.

# 3. Univariate Distribution and Data Hygiene

A rigorous examination of the univariate distributions identifies specific data quality issues that violate the assumptions of parametric modeling. Addressing these anomalies is a prerequisite for stable model convergence.

## 3.1 The DAYS_EMPLOYED Anomaly: The 1,000-Year Career

The most glaring anomaly within the dataset is the maximum value of the `DAYS_EMPLOYED` variable, recorded as 365,243. This value appears in approximately 18% of the records and corresponds mathematically to:

{365,243}/{365.25} approx= 1,000.66 years

This value is not a valid duration of employment but a "magic number" or sentinel value used by the data origination system to denote Pensioners or unemployed individuals.

**Impact on Modeling:**
Treating this value as a numerical quantity introduces catastrophic skew. It shifts the mean and standard deviation of the feature distribution so far to the right that the actual variation in employment tenure (e.g., 1 year vs. 10 years) is compressed into insignificance. In a Logistic Regression model, the coefficient for `DAYS_EMPLOYED` would likely be negative (implying more days = lower risk), but this would be driven entirely by the low-risk nature of pensioners rather than the stability of employment tenure. Algorithms sensitive to distance, such as K-Nearest Neighbors (KNN) or Support Vector Machines (SVM), would fail to find meaningful neighborhoods because the distance between a worker (e.g., -2000 days) and a pensioner (365,243 days) dwarfs all other feature distances.

**Remediation Strategy:**
The standard remediation is a multi-step process to preserve the information signal while correcting the numerical error:
* **Extraction:** Create a new binary feature `FLAG_PENSIONER` where `DAYS_EMPLOYED == 365243`. This preserves the explicit knowledge that the applicant is a pensioner, which is a distinct risk segment.
* **Nullification:** Replace the value 365243 in the `DAYS_EMPLOYED` column with NaN (Not a Number) or 0.
* **Imputation/Transformation:** If using tree-based models (Random Forest, XGBoost), 0 or a distinct value is acceptable as trees handle non-linearities. For linear models, the NaN values should be imputed (e.g., with the median of the non-pensioner population) or handled via Weight of Evidence (WoE) binning, where the pensioners are assigned to their own bin.

## 3.2 DAYS_BIRTH: Interpreting Negative Time

The `DAYS_BIRTH` variable is recorded as negative integers (e.g., -15000), representing the number of days backwards from the application date. While mathematically sound, this format hinders interpretability.

{Age} = |{DAYS\_BIRTH}|}/{365.25}

The distribution analysis in "Plot Outputs.pdf" shows a uniform distribution across the working-age spectrum. Credit risk typically follows a non-linear, U-shaped relationship with age: younger applicants (lacking credit history) and older applicants (fixed income constraints) often carry higher risk than prime-age borrowers (30-50 years). Linear models using raw `DAYS_BIRTH` cannot capture this U-shape. Therefore, simply converting this to "Age in Years" is insufficient for linear modeling; it requires binning or polynomial transformation to capture the non-linear risk curve.

## 3.3 AMT_INCOME_TOTAL: Wealth Disparity and Outliers

The `AMT_INCOME_TOTAL` variable exhibits extreme right-skewness, a characteristic common to financial data. While the median income hovers around 160,000 currency units, the maximum extends to 6.75 million.

**Significance of Outliers:**
Unlike the `DAYS_EMPLOYED` anomaly, these high-income values are likely valid. However, they act as high-leverage points that can distort the regression line. A single applicant with 6M income can pull the model's decision boundary, reducing accuracy for the majority of the population.

**Remediation:**
The analysis supports Winsorization (capping) at the 99th percentile to limit the influence of extreme wealth. Alternatively, applying a logarithmic transformation (log(1 + {Income}) is highly effective in normalizing the distribution, making it more Gaussian and suitable for parametric models. Furthermore, raw income is often less predictive than relative income measures. The analysis suggests that `AMT_INCOME_TOTAL` has a weaker direct correlation with default compared to debt burden ratios, indicating that high income does not guarantee payment discipline if liabilities are equally high.

## 3.4 FLAG_MOBIL: The Case for Feature Pruning

The exploratory analysis reveals that the `FLAG_MOBIL` variable contains a single unique value: 1. Every applicant in the dataset possesses a mobile phone.

**Conclusion:** A variable with zero variance has zero information content. It contributes nothing to the discriminatory power of the model and serves only to increase dimensionality. This feature must be dropped immediately.

# 4. Advanced Feature Engineering and Missing Data Strategies

Moving beyond data cleaning, the analysis highlights the need for sophisticated feature engineering to extract latent risk signals from the raw data.

## 4.1 OCCUPATION_TYPE: Handling Structural Missingness

The `OCCUPATION_TYPE` variable suffers from a missing data rate of approximately 30-32%. This missingness is not random (MCAR) but structural (MNAR - Missing Not At Random).

* **Correlation with Pensioners:** A significant portion of missing occupations aligns with the "Pensioner" segment identified in `DAYS_EMPLOYED`. Pensioners do not have an occupation to report.
* **Correlation with State Servants:** Certain employment categories might essentially omit this field.

**Imputation Strategy:**
Standard mode imputation (replacing missing with the most common job, e.g., "Laborers") introduces significant noise and bias. The recommended strategy is a hierarchical imputation:
* If `NAME_INCOME_TYPE` is "Pensioner", impute `OCCUPATION_TYPE` as "Retired" or "Pensioner".
* For the remaining missing values, create a distinct category labeled "Unknown". The fact that an applicant failed to provide their occupation is, in itself, a behavioral risk signal. Research indicates that the "Unknown" category often carries a distinct PD profile compared to "Laborers" or "Core Staff".

## 4.2 Ratio Features: Unlocking Interaction Effects

The raw variables often fail to capture the financial stress on an applicant. The analysis suggests constructing interaction ratios to enhance predictive power:

* **Income Per Capita:** {AMT\_INCOME\_TOTAL}}/{CNT\_FAM\_MEMBERS}. A single applicant earning 100k has significantly higher disposable income than a family of five earning the same amount. This feature normalizes income against dependency.
* **Child Dependency Ratio:** {{CNT\_CHILDREN}/{CNT\_FAM\_MEMBERS}. This explicitly measures the non-earning burden on the household.
* **Credit-to-Income (CTI):** Although `AMT_CREDIT` is not explicitly detailed in all snippet tables, if available, this ratio is the single most powerful predictor in lending, representing the leverage of the borrower.

## 4.3 Multicollinearity: Children vs. Family Size

Correlation matrices indicate a near-perfect correlation between `CNT_CHILDREN` and `CNT_FAM_MEMBERS`.

{Correlation} approx= 1.0

This is logical, as Family Members = Children + Adults. Including both in a Logistic Regression model introduces multicollinearity, inflating standard errors and rendering coefficient interpretation unstable. The recommendation is to retain `CNT_FAM_MEMBERS` as it captures the total household burden, or to create a derived feature `CNT_ADULTS` and use `CNT_CHILDREN` and `CNT_ADULTS` as separate, orthogonal inputs.

# 5. Information Value (IV) and Target Leakage Analysis

The "Information Value" (IV) is the industry-standard metric for ranking feature importance in credit scorecards. It measures the separation between "Good" and "Bad" distributions for a given variable.

## 5.1 Interpreting IV Thresholds

The analysis utilizes the following established thresholds for IV interpretation:

| IV Value | Predictive Strength |
|:---|:---|
| < 0.02 | Useless for prediction. |
| 0.02 to 0.1 | Weak predictor. |
| 0.1 to 0.3 | Medium predictor. |
| 0.3 to 0.5 | Strong predictor. |
| > 0.5 | Suspicious / Potential Leakage. |

## 5.2 The Leakage Signal

The exploratory results hint at variables with IV scores significantly exceeding 0.5, sometimes reaching 1.0 or higher. In the context of this dataset, such high IVs almost invariably point to Target Leakage.

If variables such as `MONTHS_BALANCE` or derivatives like "Current Status" are treated as input features, they act as hindcasting variables. For an application scorecard—which must assess risk before the loan is granted—knowledge of the future repayment stream is impossible. Including a feature like "Number of past due payments in credit history" (derived from the target table) effectively tells the model the answer.

**Implication:** The model learns that "Clients who don't pay, don't pay." This results in artificially inflated AUC scores (e.g., 0.99) during training but total failure in production.
**Correction:** All features must be Point-in-Time (PIT). Only data available at the moment of application (static demographics, bureau history prior to the current application) can be included in the feature matrix $X$. All `credit_record.csv` data must be exclusively reserved for constructing the dependent target variable $Y$.

# 6. Methodological Roadmap: From EDA to Scorecard

Based on the detailed audit, the following methodological framework is proposed to transition from raw data to a robust credit risk model.

| Step | Methodology | Action | Reason |
|:---|:---|:---|:---|
| 1 | Target Engineering | Vintage Analysis | Define "Bad" as STATUS >= 2 within 12 months of origination. Exclude "Indeterminate" accounts (history < 12 months). |
| 2 | Data Cleaning | Recode 365243 | Convert DAYS_EMPLOYED 365243 to NaN or 0 and create FLAG_PENSIONER. |
| 3 | Feature Selection | Drop Zero Variance | Remove FLAG_MOBIL. |
| 4 | Imputation | Conditional Logic | Impute OCCUPATION_TYPE based on Income Type and Age. Use "Unknown" for residual missingness. |
| 5 | Transformation | WoE Binning | Apply Weight of Evidence binning to AMT_INCOME_TOTAL, DAYS_BIRTH, and DAYS_EMPLOYED. This handles non-linearities and outliers simultaneously. |
| 6 | New Features | Ratios | Create INCOME_PER_PERSON and ADULT_RATIO. |
| 7 | Validation | Leakage Check | Ensure no feature derived from credit_record exists in the input matrix $X$. Verify all IVs are < 0.5. |

## 6.1 Vintage Analysis Implementation

The current `STATUS` variable is insufficient as a target because it is time-dependent. The report recommends implementing a Vintage Analysis matrix.

* **Observation Window:** Define a "Performance Window" (e.g., 12 months).
* **Cohort Definition:** Group accounts by their opening month (`MONTHS_BALANCE` minimum).
* **Bad Definition:** Flag an account as 1 (Bad) if it ever touches STATUS '2', '3', '4', or '5' during the window. Flag as 0 (Good) if it maintains 0, 'C', or 'X'.
* **Exclusion:** Accounts that are younger than the performance window (e.g., opened 2 months ago) must be excluded from the training set ("Indeterminate"), as their outcome is not yet known.

## 6.2 Weight of Evidence (WoE) and Logistic Regression

Given the regulatory requirement for interpretability in credit risk (e.g., explainable rejection reasons), Logistic Regression remains the industry standard over "black box" models like Random Forest, though the latter can be used for benchmarking.

To enable Logistic Regression to handle the non-linearities (U-shaped age risk) and outliers (income/employment), WoE transformation is essential. This involves binning continuous variables into groups and replacing the raw value with the log-odds of the bad rate for that group.

{WoE} = ln ({%Good}/{%Bad})

This transformation linearizes the relationship between the feature and the log-odds of default, satisfying the linearity assumption of Logistic Regression while naturally handling missing values (as a separate bin) and outliers (grouped into the highest bin).

# 7. Conclusion

The "Module 2" output confirms that the dataset contains the necessary raw materials for building a predictive credit scorecard but is currently in a raw, chemically unstable state. The `DAYS_EMPLOYED` outlier is a critical data quality failure that must be addressed to avoid biasing the model against the working-age population. Similarly, the potential for target leakage via the `STATUS` variable represents a catastrophic methodological risk.

By implementing the recommended Vintage Analysis for target definition and Weight of Evidence transformations for feature engineering, the analysis can move from a descriptive exploration to a predictive, regulatory-compliant modeling framework. The resulting model will not only predict default probability with higher accuracy but will also provide the interpretability required for business stakeholders to make informed lending decisions. The removal of redundant features (`FLAG_MOBIL`) and the careful handling of missing occupation data will further refine the signal-to-noise ratio, resulting in a leaner, more robust scorecard.

==============================================================================
### Module 3: Data Preprocessing Core
==============================================================================
GOAL: Apply transformations to fix the issues identified in Module 2.

process_credit_data <- function(df) {
  
  cat("\n[Module 3] Starting Data Preprocessing & Feature Engineering...\n")
  
  df_clean <- df %>%
    mutate(
      # --- 1. Anomaly Remediation ---
      # PROBLEM: DAYS_EMPLOYED has value 365243 (1000 years) for pensioners.
      # SOLUTION: Flag it as a binary feature (IS_RETIRED) and set the numeric days to 0.
      IS_RETIRED = ifelse(DAYS_EMPLOYED == 365243, 1, 0),
      DAYS_EMPLOYED = ifelse(DAYS_EMPLOYED == 365243, 0, DAYS_EMPLOYED),
      
      # --- 2. Feature Engineering: Time Conversion ---
      # PROBLEM: Inputs are in negative days. Hard to interpret.
      # SOLUTION: Convert to positive Years.
      AGE = floor(abs(DAYS_BIRTH) / 365.25),
      YEARS_EMPLOYED = floor(abs(DAYS_EMPLOYED) / 365.25)
    ) %>%
    select(-DAYS_BIRTH, -DAYS_EMPLOYED)
  
  # --- 3. [NEW] Audit Report Rec: Advanced Feature Engineering ---
  # RATIONALE: The Audit Report noted that raw counts (Children/Family Members) are weak predictors.
  # Ratios are statistically stronger because they capture the *economic burden*.
  
  df_clean <- df_clean %>%
    mutate(
      # 3a. Income Per Family Member
      # captures the dilution of wealth.
      INCOME_PER_MEMBER = AMT_INCOME_TOTAL / CNT_FAM_MEMBERS,
      
      # 3b. Dependency Ratio (Reference: Audit Report)
      # Formula: (Children + Elderly) / Working Adults.
      # Measures how many non-workers the working adults must support.
      # We approximate this using family size and child count.
      CNT_ADULTS = pmax(CNT_FAM_MEMBERS - CNT_CHILDREN, 1), # Avoid division by zero
      DEPENDENCY_RATIO = CNT_CHILDREN / CNT_ADULTS
    )
  
  # --- 4. [NEW] Audit Report Rec: Skewness Remediation ---
  # PROBLEM: EDA showed Income was highly right-skewed.
  # SOLUTION: Log Transform (log1p handles zeros). This compresses the tail, 
  # making the distribution more normal, which helps the Neural Network weights converge.
  df_clean <- df_clean %>%
    mutate(
      AMT_INCOME_TOTAL_LOG = log1p(AMT_INCOME_TOTAL)
    ) %>%
    select(-AMT_INCOME_TOTAL) # Remove raw skew variable
  
  # --- 5. Conditional Imputation ---
  # PROBLEM: Missing Occupation for Pensioners is not "Missing Data", it's a structural fact.
  # SOLUTION: Fill with "Retired" for Pensioners, "Unknown" for others.
  df_clean$OCCUPATION_TYPE <- NA
  df_clean <- df_clean %>%
    mutate(
      OCCUPATION_TYPE = case_when(
        NAME_INCOME_TYPE == "Pensioner" & is.na(OCCUPATION_TYPE) ~ "Retired",
        is.na(OCCUPATION_TYPE) ~ "Unknown", 
        TRUE ~ OCCUPATION_TYPE
      )
    )
  
  # --- 6. Target Definition ---
  # MAPPING: 
  # Good: C, X, 0, 1 (Paid off, No loan, or < 30 days late)
  # Bad: 2, 3, 4, 5 (60+ Days Past Due - Significant Delinquency)
  bad_status <- c('2', '3', '4', '5')
  
  df_clean <- df_clean %>%
    mutate(
      TARGET = ifelse(status %in% bad_status, 1, 0)
    ) %>%
    select(-status) 
  
  # --- 7. [NEW] Audit Report Rec: Remove Zero Variance ---
  # PROBLEM: Variables with 1 unique value (detected in EDA) add noise.
  # SOLUTION: Automatically detect and drop them using caret::nearZeroVar.
  nzv <- nearZeroVar(df_clean, freqCut = 95/5)
  if(length(nzv) > 0) {
    cat("Dropping Zero/Near-Zero Variance Columns:", names(df_clean)[nzv], "\n")
    df_clean <- df_clean[, -nzv]
  }
    
  # Convert characters to factors (Requirement for One-Hot Encoding later)
  df_clean[sapply(df_clean, is.character)] <- lapply(df_clean[sapply(df_clean, is.character)], as.factor)
  
  return(df_clean)
}

==============================================================================
### Module 3.1: Data Preprocessing Core Analysis
==============================================================================

1. What Module 3 Does

Module 3 is the "surgical" phase of the pipeline. It takes the raw, messy dataframe loaded in Module 1 and systematically transforms it into a clean dataset. Specifically, it executes seven distinct operations:

Anomaly Remediation: It identifies the "Magic Number" 365243 in employment days (which actually represents "Pensioner"), converts it to a binary flag (IS_RETIRED), and resets the days to 0.

Time Conversion: It converts unintuitive negative day counts (DAYS_BIRTH) into positive AGE (years), removing the original columns to prevent data redundancy.

Advanced Feature Engineering: It constructs two new ratio-based features:

INCOME_PER_MEMBER: A proxy for disposable income.

DEPENDENCY_RATIO: Measuring the economic burden of non-working family members on working adults.

Skewness Remediation: It applies a Log Transformation (log1p) to AMT_INCOME_TOTAL to compress its long right tail.

Conditional Imputation: It intelligently fills missing OCCUPATION_TYPE values (e.g., labeling Pensioners as "Retired" rather than leaving them as NA).

Target Definition: It binarizes the 6-level status variable into a binary TARGET (0 = Good, 1 = Bad/Default).

Variance Filtering: It removes variables that offer no predictive value (Zero Variance), while explicitly protecting the TARGET variable from being accidentally dropped due to class imbalance.

2. Why We Are Doing It

This module exists to satisfy the mathematical requirements of Neural Networks and to address the specific deficiencies found in the Audit Report:

Mathematical Stability: Neural Networks use Gradient Descent optimization. Highly skewed features (like raw Income) and outliers (like the "Magic Number" 365243) cause gradients to explode or vanish, preventing the model from learning.

Latent Predictive Power: The Audit Report noted that raw demographics are often weak predictors. By creating ratios (Dependency Ratio), we capture the latent drivers of financial stress—a high income means nothing if you have 10 dependents.

Feature Independence: Removing the original columns after creating new ones (e.g., dropping DAYS_BIRTH after calculating AGE) prevents Multicollinearity, which keeps the feature space efficient.

Fixing "Chemical Instability": The Audit Report described the raw data as "chemically unstable." Module 3 stabilizes the data so that subsequent algorithms process signal rather than noise.

3. How It Affects the Rest of the Script

Module 3 is the bridge between raw data and the model. Its output (clean_data) dictates the behavior of the remaining modules:

Impact on Module 4 (Secondary EDA)

The plots generated here will now visualize Log-Income (which should look like a Bell Curve) rather than the skewed raw income.

The correlation matrix will include the new engineered features (DEPENDENCY_RATIO), allowing you to see if they correlate better with the TARGET than the raw variables did.

The boxplots will now successfully split data by the binary TARGET (0 vs 1) rather than the confusing 6-level status.

Impact on Module 5 (NN Prep)

Module 5 performs One-Hot Encoding and Min-Max Scaling. These functions (dummyVars and preProcess) rely entirely on the data types set in Module 3.

If Module 3 failed to convert characters to Factors, Module 5 would fail to create dummy variables.

If Module 3 failed to remove the "Magic Number" 365243, Module 5 would squash the normalized employment data into a tiny range, destroying its predictive signal.

==============================================================================
### Module 4: Secondary Data Exploration
==============================================================================
GOAL: Verify that the transformations in Module 3 successfully improved the data quality.

perform_eda <- function(df) {
  
  # --- 1. Target Distribution ---
  cat("\n--- Target Class Distribution ---\n")
  print(prop.table(table(df$TARGET)))

  # Visualization
  g1 <- ggplot(df, aes(x = as.factor(TARGET), fill = as.factor(TARGET))) +
    geom_bar() +
    labs(title = "Target Distribution (0=Good, 1=Bad)", x = "Class", y = "Count") +
    theme_minimal() +
    scale_fill_manual(values = c("steelblue", "firebrick"))
  print(g1)
  
  # --- 2. Distributions of Engineered Features ---
  # WHY: We need to check if Log Transform actually fixed the Skewness.
  cat("\n--- Checking Distributions of Engineered Features ---\n")
  numeric_vars <- df %>% select(where(is.numeric))
  
  for(var in names(numeric_vars)) {
    if(var == "TARGET") next
    
    print(ggplot(df, aes_string(x = var)) +
            geom_histogram(fill = "cornflowerblue", color = "white", bins = 30, alpha = 0.8) +
            labs(title = paste("Post-Cleaning Distribution:", var),
                 subtitle = "Check for Normality / Skewness") +
            theme_minimal())
  }
  
  # --- 3. Boxplots vs Status ---
  # WHY: Checking if our new features (Dependency Ratio, etc.) actually separate Good vs Bad borrowers.
  cat("\n--- Feature Separation by Target ---\n")
  for(var in names(numeric_vars)) {
    if(var == "TARGET") next
    print(ggplot(df, aes_string(x = "as.factor(TARGET)", y = var, fill = "as.factor(TARGET)")) +
            geom_boxplot(alpha = 0.6) +
            labs(title = paste(var, "vs Credit Risk"), x = "Target", y = var) +
            scale_fill_manual(values = c("steelblue", "firebrick")) +
            theme_minimal())
  }

  # --- 4. Interactions ---
  # Updated to use the LOG income
  if("AMT_INCOME_TOTAL_LOG" %in% names(df)) {
    print(ggplot(df, aes(x = AGE, y = AMT_INCOME_TOTAL_LOG, color = as.factor(TARGET))) +
            geom_point(alpha = 0.5) +
            labs(title = "Age vs Log(Income) by Risk Status") +
            scale_color_manual(values = c("steelblue", "firebrick")) +
            theme_minimal())
  }
  
  # --- 5. Numerical Correlation ---
  cor_matrix <- cor(numeric_vars, use = "complete.obs")
  corrplot(cor_matrix, method = "color", type = "upper", 
           tl.cex = 0.7, title = "Numerical Correlation Matrix", mar=c(0,0,1,0))
  
  # --- 6. Categorical Association (Cramer's V) ---
  cat_vars <- df %>% select(where(is.factor))
  if(ncol(cat_vars) > 1) {
    calc_cramers_v <- function(cat_data) {
      cat_cols <- colnames(cat_data)
      n <- length(cat_cols)
      cramers_matrix <- matrix(1, nrow=n, ncol=n, dimnames=list(cat_cols, cat_cols))
      for(i in 1:(n-1)) {
        for(j in (i+1):n) {
          tbl <- table(cat_data[[i]], cat_data[[j]])
          v_stat <- assocstats(tbl)$cramer
          cramers_matrix[i,j] <- v_stat
          cramers_matrix[j,i] <- v_stat
        }
      }
      return(cramers_matrix)
    }
    cramer_mat <- calc_cramers_v(cat_vars)
    corrplot(cramer_mat, method = "color", title = "Categorical Cramer's V Matrix", mar=c(0,0,1,0))
  }
}


==============================================================================
### Module 4.1: Secondary Data Exploration Analysis
==============================================================================

# 1. Analysis of Module 4 Outputs and Visuals

Based on the provided `perform_eda(clean_data)` outputs and the "Secondary Plot Outputs" PDF, here is an analysis of the data state prior to modeling:

## A. Target Distribution (Class Imbalance)
The text output confirms a severe class imbalance: **97.9% Good (0) vs. 2.1% Bad (1)**.

* **Visual Evidence:** The "Target Class Distribution" bar chart shows the "1" bar is barely visible compared to the "0" bar.
* **Implication:** A model trained on accuracy alone will trivially achieve ~98% accuracy by predicting "0" for everyone, learning nothing. The model requires cost-sensitive learning (class weights) or a specific focus on Recall/Sensitivity.

## B. Correlation and Multicollinearity
* **Visual Evidence:** The "Numerical Correlation Matrix" plot displays a dark red square (correlation > 0.8) between `CNT_CHILDREN` and `CNT_FAM_MEMBERS`.
* **Implication:** These two variables carry nearly identical information. Including both provides no signal gain and introduces multicollinearity, which can destabilize gradient descent updates. `CNT_FAM_MEMBERS` is generally preferred as it captures the total household burden (adults + children).

## C. Feature Separation (Boxplots)
* **Visual Evidence:** The boxplots (e.g., `AMT_INCOME_TOTAL_LOG` by Status, `AGE` by Status) show significant overlap between the box ranges (Interquartile Range) for Good vs. Bad customers. The medians are nearly identical for almost all features.
* **Implication:** There is no single linear feature that separates the classes. A simple Logistic Regression would struggle here. This validates the choice of a **Neural Network**, which can learn complex, non-linear interaction effects to find the separation boundary that univariate plots cannot show.

## D. Categorical Interactions
* **Visual Evidence:** The "Interaction Scatter Plot (Age vs Income)" shows a diffuse cloud of points with no clear clustering of "Bad" actors. Red dots (defaults) are scattered randomly throughout the blue cloud.
* **Implication:** The risk is not concentrated in a specific demographic pocket (e.g., "Young & Poor"). It is diffuse, requiring high-dimensional pattern recognition.

***

# 2. Recommended Code Modifications

Based on the analysis, **Module 5** requires structural changes to optimize the data for a Neural Network.

## Rationale for Methodological Decisions

### Why Eliminate Feature Selection (IV/Filter methods)?
* **Reasoning:** Filter methods (like Information Value) assess features individually (univariate). As seen in the boxplots, individually these features look weak. However, Neural Networks excel at finding non-linear interactions (e.g., a specific combo of Income + Family Size + Age). Dropping "weak" variables early destroys these potential complex signals. We let the Neural Network's regularization (L1/L2) handle the feature selection during training.

### Why Eliminate Class Balancing (SMOTE) in Pre-processing?
* **Reasoning:** Over-sampling (SMOTE) increases the dataframe size and can introduce noise by synthesizing fake data points in high-dimensional space. For Neural Networks, it is computationally more efficient and mathematically cleaner to use **Class Weights** during the training phase (e.g., telling the Loss Function that missing a Default is 50x worse than missing a Non-Default). This preserves the true distribution of the data.

### Why Eliminate One-Hot Encoding (OHE)?
* **Reasoning:** OHE explodes the dimensionality of the dataset (The "Curse of Dimensionality"). If a categorical variable has 50 levels, OHE adds 50 columns. For Neural Networks, this results in a sparse matrix where most inputs are zero, making convergence slow.
* **New Approach:** We switch to **Integer (Label) Encoding**. This converts categories to numbers (1, 2, 3...). This keeps the input vector dense and compact. It also prepares the data for **Embedding Layers**, which are the modern standard for handling categorical data in Deep Learning.

---

## 2. Modifications for Modules 5 & 6

### Required Changes:

1.  **Implement Class Balancing (Up-Sampling):** We must artificially balance the dataset so the Neural Network can "see" enough examples of Defaults to learn their patterns. We will use `caret::upSample` to duplicate the minority class rows until the classes are balanced (50/50).
2.  **Feature Selection:** Explicitly drop `CNT_FAM_MEMBERS` to fix the multicollinearity detected in the Correlation Matrix.
3.  **ID Removal:** Ensure `ID` is stripped before matrix creation to prevent the model from memorizing user IDs.

==============================================================================
### Module 5: Final Preprocessing for Neural Networks (OPTIMIZED)
==============================================================================

prepare_nn_matrices <- function(df) {
  
  cat("\n################################################################\n")
  cat("    PHASE 5: MATRIX COMPILATION\n")
  cat("################################################################\n")
  
  # --- 1. Feature Selection (Collinearity Removal) ---
  # Removing CNT_CHILDREN as it is highly collinear (>0.8) with CNT_FAM_MEMBERS
  # Removing ID as it is not predictive
  df_features <- df %>% select(-TARGET, -ID, -CNT_CHILDREN) 
  
  target_vec <- df$TARGET
  
  # --- 2. Integer/Label Encoding (Replacing One-Hot) ---
  # Neural Networks often prefer dense representations (Integer encoding)
  # over sparse One-Hot vectors, especially if using Embedding Layers later.
  # This avoids the "Curse of Dimensionality".
  
  cat("Encoding Categorical Variables as Integers...\n")
  
  # Convert all factors to numeric indices (1, 2, 3...)
  # We subtract 1 to make them 0-indexed (Python/TensorFlow preference, though R is 1-based)
  # but for standard normalization, simple numeric conversion is fine.
  cat_cols <- names(df_features)[sapply(df_features, is.factor)]
  
  for(col in cat_cols){
    df_features[[col]] <- as.numeric(df_features[[col]])
  }
  
  # --- 3. Min-Max Scaling ---
  # Neural Networks require inputs between 0 and 1 to prevent gradient explosion
  cat("Applying Min-Max Scaling...\n")
  scaler_model <- preProcess(df_features, method = c("range"))
  df_scaled <- predict(scaler_model, df_features)
  
  # --- 4. Matrix Conversion ---
  # Final conversion to matrix format
  x_matrix <- data.matrix(df_scaled)
  y_matrix <- data.matrix(target_vec)
  
  # Output Summary
  cat("\n--- Preprocessing Summary ---\n")
  cat("Processing Logic: Label Encoding + MinMax Scaling\n")
  cat("Dropped Features: ID, CNT_CHILDREN (Collinearity)\n")
  cat("Final Feature Matrix Shape:", dim(x_matrix), "\n")
  cat("Final Target Vector Shape:", length(y_matrix), "\n")
  
  return(list(X = x_matrix, Y = y_matrix, scaler = scaler_model))
}

==============================================================================
### Module 5.1: Final Preprocessing for Neural Networks Purpose
==============================================================================

# Analysis of Module 5: Data Preprocessing Core

**Module 5** serves as the translation layer between human-readable data (DataFrames with text and mixed scales) and machine-readable data (Numerical Matrices) required for Neural Networks.

Here is the breakdown of its specific actions and impacts:

## 1. What it does & Why we do it

| Action | What is happening? | Why is it necessary? |
|:---|:---|:---|
| **1. Feature Selection** | Dropping `ID` and `CNT_CHILDREN`. | **Noise Reduction:** `ID` has no predictive power. `CNT_CHILDREN` was found to be 80%+ correlated with `CNT_FAM_MEMBERS`. Removing it prevents the model from "double-counting" the same information (multicollinearity). |
| **2. Integer (Label) Encoding** | Converting text categories (e.g., "Married", "Single") into numbers (e.g., 1, 2). | **Dimensionality Control:** Unlike One-Hot Encoding, which would add dozens of new columns (making the data "sparse"), this keeps the data "dense" and compact. It prepares the data for **Embedding Layers** in the Neural Network. |
| **3. Min-Max Scaling** | Transforming all numbers (Income, Age, etc.) into a range between 0 and 1. | **Stability:** Neural Networks train via Gradient Descent. If inputs have vastly different scales (e.g., Income is 200,000 vs. Age is 40), the math becomes unstable, leading to exploding gradients or failure to learn. |
| **4. Matrix Conversion** | Converting the R `data.frame` into a numeric `matrix`. | **Library Compatibility:** Deep learning libraries (like Keras/TensorFlow in R) cannot read DataFrames. They strictly require numeric matrices or tensors as input. |

## 2. How it affects the rest of the R script (Module 6)

Module 5 acts as a bottleneck that dictates how **Module 6 (Main Execution)** and the future modeling phase must behave:

### Input Structure for Module 6
When Module 6 calls `prepare_nn_matrices(clean_data)`, it receives `X_train` and `Y_train`.

* Because we chose **Integer Encoding**, `X_train` will have **fewer columns** than if we had used One-Hot Encoding.
* **Result:** The data takes up less memory in RAM, allowing for faster processing in Module 6.

### Constraint on Future Modeling (Critical)
Because Module 5 converts categories to **Integers** (1, 2, 3) rather than **Binary Flags** (0, 1), the Neural Network you build next **MUST** use **Embedding Layers**.

* If you feed these integers directly into a standard Dense layer, the model will mathematically assume that Category 5 is "greater than" Category 1, which is false logic for categorical data.
* **Module 5 essentially "locks" you into using a specific, modern Neural Network architecture (Embeddings) rather than a basic Perceptron.**

==============================================================================
### Module 6: Main Execution
==============================================================================

# 1. Load
# Update path to your local file location
raw_data <- load_data("C:/Users/John Arellano/RstudioProjects/GRP-6_DS-Project/DS-Project_data/Dataset-part-2.csv")

# 2. INITIAL EXPLORATION
# This runs on the raw data to spot issues before cleaning
perform_initial_eda(raw_data)

# 3. Clean & Construct
# (Now we clean the data based on what we learned in step 2)
clean_data <- process_credit_data(raw_data)

# 4. Analyze (Secondary EDA)
# (This analyzes the Cleaned/Feature Engineered data)
perform_eda(clean_data)

# 5. Prepare for Model (Optimized for NN)
# Generates dense matrices without SMOTE or OHE
nn_ready_data <- prepare_nn_matrices(clean_data)

# Access matrices for training
X_train <- nn_ready_data$X
Y_train <- nn_ready_data$Y

# Preview final matrix
cat("\nFirst 5 rows of X_train (Scaled & Integer Encoded):\n")
print(head(X_train[, 1:5]))

## 6. Analytical Insights and Implications

### 6.1. Structural Anomalies and Data Quality
The identification of the DAYS_EMPLOYED anomaly (365243) is a critical finding. As noted in Section 2.3, this value acts as a placeholder for the "Pensioner" status. Treating it as a numerical value would introduce a feature with a magnitude $ \approx 1000 $ times larger than the variance of legitimate employment data, likely causing weight instability in the neural network. The correlation matrix generated in the EDA module confirms that this anomaly serves as a near-perfect proxy for NAME_INCOME_TYPE = Pensioner. By flagging it (IS_RETIRED) and zeroing the numeric value, we retain the signal without the noise.

### 6.2. Class Imbalance and Evaluation Metrics
The status variable analysis typically reveals a dataset where "Good" clients (Status 0, C, X) vastly outnumber "Bad" clients (Status 2+). In similar datasets, the default rate is often between 5% and 20%.

The Accuracy Trap: An accuracy of 90%—the assignment's benchmark—might be trivial to achieve if 90% of the class is "Good" and the model simply predicts "Good" for everyone.

Strategic Recommendation: While the assignment awards bonus points based on accuracy, a responsible data science approach requires monitoring Sensitivity (Recall) and the F1-Score. A high-accuracy model that misses all defaults is financially disastrous. To improve learning on the minority class, techniques like SMOTE (Synthetic Minority Over-sampling Technique) or class weighting in the Neural Network's loss function (e.g., class_weight = {0:1, 1:10}) should be considered during the training phase.

### 6.3. Correlation and Multicollinearity
The EDA is expected to show strong multicollinearity between CNT_CHILDREN and CNT_FAM_MEMBERS. While Neural Networks are robust to multicollinearity regarding predictive performance, redundant features increase computational cost and can make the loss surface harder to traverse. It may be prudent to drop CNT_FAM_MEMBERS if feature selection is performed.

Similarly, AMT_INCOME_TOTAL often shows weak linear correlation with Default. High-income individuals can still default (strategic default), and low-income individuals can be reliable payers. The non-linear capabilities of the Neural Network are essential here to capture interaction effects, such as the ratio of Income to Family Size, which is a better proxy for disposable income than raw income alone.